# üî¨ DEEP DIVE COMPLETO ‚Äî AN√ÅLISIS DE TODOS LOS RECURSOS

> **Fecha:** 2026-02-20  
> **Objetivo:** Scrapear p√°ginas, encontrar lo m√°s barato, comparar precios, y revender con margen  
> **M√©todo:** Lectura chunk-por-chunk de CADA recurso, sin saltear nada

---

## üìã √çNDICE DE RECURSOS ANALIZADOS

| # | Recurso | URL | Chunks Le√≠dos | Relevancia |
|---|---------|-----|---------------|------------|
| 1 | **skills.sh** | https://skills.sh | 100+ chunks | üî¥ CR√çTICA |
| 2 | **agents.md** | https://agents.md | 7 chunks | üü° MEDIA |
| 3 | **curl_cffi** | https://curl-cffi.readthedocs.io | 30+ chunks | üî¥ CR√çTICA |
| 4 | **MCP (Model Context Protocol)** | https://modelcontextprotocol.io | 15+ chunks | üü° MEDIA |
| 5 | **Claude Code Docs** | https://docs.anthropic.com/en/docs/claude-code | 20+ chunks | üü° MEDIA |
| 6 | **leaked-system-prompts** | https://github.com/jujumilk3/leaked-system-prompts | 5 chunks | üü¢ BAJA |

---

## 1Ô∏è‚É£ SKILLS.SH ‚Äî Ecosistema de Skills para AI Agents

### ¬øQu√© es?
Un marketplace/ecosistema abierto de "Skills" (capacidades reutilizables) para agentes de IA. Cada skill es un paquete con instrucciones, scripts y assets que extienden las capacidades de un agente.

### Estructura de una Skill
```
skill-name/
‚îú‚îÄ‚îÄ SKILL.md           # Instrucciones principales (OBLIGATORIO)
‚îú‚îÄ‚îÄ scripts/           # Scripts helper
‚îú‚îÄ‚îÄ references/        # Documentaci√≥n de referencia
‚îú‚îÄ‚îÄ assets/            # Archivos adicionales
‚îî‚îÄ‚îÄ examples/          # Ejemplos de uso
```

### Skills Analizadas ‚Äî TIER 1 (Impacto Directo)

#### 1.1 `systematic-debugging` (obra/superpowers)
- **Iron Law:** `NO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST`
- **4 Fases:** Root Cause Investigation ‚Üí Pattern Analysis ‚Üí Hypothesis+Test ‚Üí Implementation
- **Multi-Layer Diagnostic:** Cuando hay m√∫ltiples componentes (API‚ÜíService‚ÜíDB), agregar logs en CADA frontera
- **Data Flow Tracing:** Rastrear hacia atr√°s hasta la fuente del valor malo
- **Regla de 3:** Si 3 fixes fallan ‚Üí parar y cuestionar la ARQUITECTURA
- **üéØ FRAVEGA:** Cuando un sniffer falla (403, timeout, datos corruptos), seguir este proceso exacto

#### 1.2 `xlsx` (anthropics/skills)
- **2 librer√≠as:** `openpyxl` para f√≥rmulas/formato, `pandas` para an√°lisis/bulk
- **Regla CR√çTICA:** SIEMPRE usar f√≥rmulas Excel (`=SUM(B2:B9)`), NUNCA hardcodear valores
- **Color Coding:** Azul=inputs, Negro=f√≥rmulas, Verde=links entre hojas, Rojo=links externos
- **Script recalc.py:** Usa LibreOffice para recalcular f√≥rmulas. Retorna JSON con ubicaciones de errores
- **openpyxl tips:** `data_only=True` lee valores pero PIERDE f√≥rmulas al guardar. `read_only=True` para archivos grandes
- **üéØ FRAVEGA:** Para leer `Precios competidores.xlsx`, crear reportes de comparaci√≥n de precios

#### 1.3 `async-python-patterns` (wshobson/agents)
- **8 Patterns:** Basic async/await, gather(), Task management, Error handling, Timeout, Semaphores, Producer-Consumer, Rate limiting
- **Semaphore Rate Limiting:** `semaphore = asyncio.Semaphore(3)` ‚Üí m√°ximo 3 requests paralelos
- **Connection Pools:** `aiohttp.TCPConnector(limit=100, limit_per_host=10)`
- **Batch Processing:** Procesar items en lotes de N con `gather()`
- **Error handling:** `asyncio.gather(*tasks, return_exceptions=True)` ‚Üí no falla todo si una task falla
- **Timeout:** `asyncio.wait_for(operation, timeout=2.0)` ‚Üí mata operaciones lentas
- **üéØ FRAVEGA:** Reemplazar scraping sync por async con rate limiting. Pasar de 1 a 3-5 requests paralelos

#### 1.4 `error-handling-patterns` (wshobson/agents)
- **Circuit Breaker:** 3 estados: CLOSED (normal) ‚Üí OPEN (falla, rechaza) ‚Üí HALF_OPEN (prueba)
```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=timedelta(seconds=60), success_threshold=2):
        self.state = CircuitState.CLOSED
```
- **Error Aggregation:** Colectar m√∫ltiples errores en vez de fallar en el primero
- **Graceful Degradation:** `with_fallback(primary=cache, fallback=db)`
- **Multiple Fallbacks:** `try_function(api1) or try_function(api2) or try_function(cache) or DEFAULT`
- **Retry con Exponential Backoff:** `@retry(max_attempts=3, backoff_factor=2.0)`
- **Custom Exception Hierarchy:** `ApplicationError` ‚Üí `ScrapingError` ‚Üí `WAFBlockedError`
- **üéØ FRAVEGA:** Circuit Breaker para API de Fr√°vega. Retry con backoff para requests.

#### 1.5 `writing-plans` + `executing-plans` (obra/superpowers)
- **Granularidad:** 2-5 minutos por paso
- **Header format requerido:**
```markdown
# [Feature Name] Implementation Plan
> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans
**Goal:** [One sentence]
**Architecture:** [2-3 sentences]
**Tech Stack:** [Key technologies]
```
- **Cada task incluye:** Archivo a modificar, qu√© cambiar, test, commit
- **üéØ FRAVEGA:** Planificar CADA nuevo target (MercadoLibre, Garbarino) con pasos at√≥micos

#### 1.6 `verification-before-completion` (obra/superpowers)
- **Iron Law:** `NO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE`
- **Gate Function:** Identificar ‚Üí Ejecutar ‚Üí LEER output ‚Üí Verificar
- **Red flags:** "Ya funciona" sin correr nada
- **üéØ FRAVEGA:** Siempre mostrar evidencia real del scraping funcionando

### Skills Analizadas ‚Äî TIER 2 (Mejoran Calidad)

#### 1.7 `brainstorming` (obra/superpowers)
- Explorar ‚Üí Preguntas (una a la vez, multiple choice) ‚Üí 2-3 propuestas ‚Üí Dise√±o ‚Üí Plan
- Output: `docs/plans/YYYY-MM-DD-<topic>-design.md`
- YAGNI ruthlessly, explorar alternativas, validaci√≥n incremental

#### 1.8 `subagent-driven-development` (obra/superpowers)
- Per task: Dispatch Implementer ‚Üí Questions? ‚Üí Implements/Tests/Commits ‚Üí Spec Review ‚Üí Code Quality Review
- 3 Templates: implementer, spec-reviewer, code-quality-reviewer
- NUNCA dispatch m√∫ltiples implementers en paralelo, NUNCA skipear reviews

#### 1.9 `dispatching-parallel-agents` (obra/superpowers)
- Cu√°ndo: 3+ fallos en subsistemas independientes
- Pattern: Identificar dominios ‚Üí Crear tasks focalizados ‚Üí Dispatch paralelo ‚Üí Review + Integrate
- Cu√°ndo NO: Fallos relacionados, shared state, exploratory debugging

#### 1.10 `test-driven-development` (obra/superpowers)
- RED-GREEN-REFACTOR estricto: ver fallar ‚Üí c√≥digo m√≠nimo ‚Üí ver pasar ‚Üí refactor
- Bug fix: SIEMPRE escribir failing test que reproduzca el bug PRIMERO

#### 1.11 `python-testing-patterns` (wshobson/agents)
- AAA Pattern: Arrange ‚Üí Act ‚Üí Assert
- Mocking: `unittest.mock.Mock()`, `patch()`, `@patch` decorator
- DB testing: `@pytest.fixture(scope="function")` con SQLite in-memory

#### 1.12 `python-performance-optimization` (wshobson/agents)
- 20 patterns: cProfile, lru_cache, generators, __slots__, multiprocessing
- Sync vs async: 4 requests de 1s: sync=4s, async=1s ‚Üí 4x speedup
- Batch DB: `executemany()` vs individual: hasta 100x speedup

#### 1.13 `prompt-engineering-patterns` (wshobson/agents)
- Structured Output con Pydantic (JSON schema enforcement)
- Chain-of-Thought con Self-Verification
- Dynamic Example Selection con semantic similarity
- Progressive Disclosure: 4 niveles de complejidad
- Token Efficiency: reducir 150+ tokens a 30 manteniendo calidad

#### 1.14 `mcp-builder` (anthropics/skills)
- 4 Fases: Deep Research ‚Üí Implementation ‚Üí Review+Test ‚Üí Evaluations
- Stack: TypeScript (preferido) o Python con FastMCP
- Zod (TS) o Pydantic (Python) para schemas

#### 1.15 `architecture-patterns` (wshobson/agents)
- Clean Architecture: domain/entities/ ‚Üí use_cases/ ‚Üí adapters/ ‚Üí infrastructure/
- Hexagonal: Core con interfaces (Ports), implementaciones externas (Adapters)
- DDD: Value Objects, Entities, Aggregates, Domain Events

#### 1.16 `api-design-principles` (wshobson/agents)
- GraphQL Schema Design completo con Relay-style pagination
- DataLoader Pattern para resolver N+1
- **üéØ FRAVEGA:** Entender EXACTAMENTE la GraphQL API de Fr√°vega

### Skills Analizadas ‚Äî TIER 3 (Para el Futuro)

| Skill | Para Qu√© | Cu√°ndo |
|-------|----------|--------|
| `docker-expert` | Deploy 24/7 del monitor | Cuando tengamos algo estable |
| `sql-optimization-patterns` | Optimizar DB cuando crezca | +10k registros |
| `github-actions-templates` | CI/CD automatizado | Cuando tengamos tests |
| `pdf` | Reportes PDF de precios | Para clientes/an√°lisis |
| `webapp-testing` | Testing de dashboards | Si hacemos UI |

---

## 2Ô∏è‚É£ AGENTS.MD ‚Äî Formato Est√°ndar para Guiar AI Agents

### ¬øQu√© es?
Un formato abierto y simple (archivo `AGENTS.md`) que funciona como "README para agentes". Un solo archivo de instrucciones que es compatible con **20+ herramientas** de IA.

### Herramientas Compatibles
- OpenAI Codex, Google Jules, GitHub Copilot agent
- Cursor, VS Code, Windsurf, Aider, RooCode
- Factory, Amp, Zed, Warp, Kilo Code, Phoenix
- Gemini CLI, goose, opencode, Devin, UiPath

### Estructura Recomendada
1. **Project overview** ‚Äî Descripci√≥n breve
2. **Build and test commands** ‚Äî `pip install`, `python -m pytest`, etc.
3. **Code style guidelines** ‚Äî Convenciones de c√≥digo
4. **Testing instructions** ‚Äî C√≥mo correr tests
5. **Security considerations** ‚Äî Reglas de seguridad

### Ejemplo Real (OpenAI Codex)
```markdown
# AGENTS.md
## Setup commands
- Install deps: `pnpm install`
- Start dev server: `pnpm dev`
- Run tests: `pnpm test`

## Code style
- TypeScript strict mode
- Single quotes, no semicolons
- Use functional patterns where possible
```

### Migraci√≥n
```bash
mv AGENT.md AGENTS.md && ln -s AGENTS.md AGENT.md
```

### Configuraci√≥n Gemini CLI
```json
{ "contextFileName": "AGENTS.md" }
```

### üéØ Aplicaci√≥n FRAVEGA
Crear un `AGENTS.md` en la ra√≠z del proyecto con:
- Setup commands (pip install, env setup)
- Estructura del proyecto
- Convenciones de c√≥digo (Python, async, naming)
- Targets activos y c√≥mo agregar nuevos
- Reglas de scraping (rate limits, headers, etc.)

---

## 3Ô∏è‚É£ CURL_CFFI ‚Äî Biblioteca Python para Bypass de WAF

### ¬øQu√© es?
Python binding para `curl-impersonate` que permite imitar el fingerprint TLS/HTTP2 de browsers reales. **LA SOLUCI√ìN** al problema de 403 que tuvimos con Fr√°vega y Flybondi.

### ¬øPor qu√© es crucial?
| Feature | requests | aiohttp | httpx | curl_cffi |
|---------|----------|---------|-------|-----------|
| HTTP/2 | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ |
| HTTP/3 | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| Sync | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |
| Async | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ |
| WebSocket | ‚ùå | ‚úÖ | ‚ùå | ‚úÖ |
| Retry nativo | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| **Fingerprints** | ‚ùå | ‚ùå | ‚ùå | **‚úÖ** |
| Velocidad | üêá | üêáüêá | üêá | üêáüêá |

### Instalaci√≥n
```bash
pip install curl_cffi --upgrade
```

### Uso B√°sico ‚Äî Impersonar Chrome
```python
import curl_cffi

# Impersonar Chrome (usa la √∫ltima versi√≥n autom√°ticamente)
r = curl_cffi.get("https://www.fravega.com", impersonate="chrome")
print(r.status_code)  # 200 en vez de 403!

# Con proxy
r = curl_cffi.get("https://www.fravega.com", 
                   impersonate="chrome", 
                   proxy="http://localhost:3128")

# Headers custom (se suman a los de Chrome)
r = curl_cffi.get("https://www.fravega.com",
                   impersonate="chrome",
                   headers={"Accept-Language": "es-AR"})

# DESACTIVAR headers default de Chrome
r = curl_cffi.get("https://www.fravega.com",
                   impersonate="chrome",
                   default_headers=False,
                   headers={"User-Agent": "Custom"})
```

### Sessions y Cookies (SIEMPRE usar)
```python
# Mantener cookies entre requests (simula navegaci√≥n real)
with curl_cffi.Session(impersonate="chrome") as s:
    # Login o primera visita
    s.get("https://www.fravega.com")
    # Segunda request con cookies del server
    r = s.get("https://www.fravega.com/api/graphql")
    print(r.json())
```

### Retry Nativo con Backoff
```python
from curl_cffi import Session, RetryStrategy

# Retry autom√°tico con exponential backoff
strategy = RetryStrategy(count=3, delay=0.2, jitter=0.1, backoff="exponential")
with Session(impersonate="chrome", retry=strategy) as s:
    r = s.get("https://www.fravega.com/api/graphql")
```

### POST con JSON (para GraphQL de Fr√°vega!)
```python
# GraphQL query a Fr√°vega
payload = {
    "query": "query { search(term: \"iphone\") { products { name price } } }"
}
r = curl_cffi.post("https://www.fravega.com/api/graphql",
                    json=payload,
                    impersonate="chrome")
```

### Async para Scraping Masivo
```python
import asyncio
from curl_cffi import AsyncSession

urls = [
    "https://www.fravega.com/l/celulares",
    "https://www.garbarino.com/celulares",
    "https://www.mercadolibre.com.ar/celulares"
]

async with AsyncSession(impersonate="chrome") as s:
    tasks = [s.get(url) for url in urls]
    results = await asyncio.gather(*tasks)
    for r in results:
        print(r.status_code, len(r.text))
```

### WebSockets (para monitoreo real-time)
```python
from curl_cffi import Session, WebSocket

def on_message(ws: WebSocket, message):
    print(f"Precio actualizado: {message}")

with Session(impersonate="chrome") as s:
    ws = s.ws_connect("wss://api.example.com/prices", on_message=on_message)
    ws.run_forever()
```

### Topics Avanzados

#### Proxies
```python
# HTTP proxy
curl_cffi.get(url, proxy="http://user:pass@proxy.com:3128")
# SOCKS proxy
curl_cffi.get(url, proxy="socks5://localhost:9050")
```

#### HTTP Version Selection
```python
# Forzar HTTP/3 (menos detecci√≥n!)
curl_cffi.get("https://www.fravega.com", http_version="v3")
# Forzar HTTP/1.1 (si h2 falla)
curl_cffi.get("https://www.fravega.com", http_version="v1")
```

#### Keep-Alive con HTTP/2
```python
s = Session(impersonate="chrome")
s.get("https://www.fravega.com")
s.upkeep()  # Manda ping frame para mantener conexi√≥n
```

### TLS Fingerprinting ‚Äî Lo Que Hace Especial a curl_cffi
- **TLS fingerprint (JA3):** Hash de los cipher suites y extensiones usados en el handshake TLS. Cada browser tiene uno fijo.
- **HTTP/2 fingerprint (Akamai):** Settings del frame HTTP/2 que identifican el browser.
- **Chrome 110+ usa ClientHello permutation:** El orden de extensiones es random ‚Üí JA3 cambia, pero ja3n (normalizado) no.
- **HTTP/3:** Menos WAFs lo usan a√∫n ‚Üí HAY MENOS DETECCI√ìN con HTTP/3.

### Impersonation FAQ (Clave!)
- **¬øC√≥mo verificar que funciona?** ‚Üí Visitar `https://tls.browserleaks.com/json` y comparar con tu browser real
- **¬øA√∫n me detectan con impersonation correcta?** ‚Üí TLS/JA3 es UN factor. Otros: IP quality, request rate, JS fingerprints. Usar proxies y rate limiting
- **¬øRandomizar fingerprints por request?** ‚Üí NO generar fingerprints random. Usar `random.choice(["chrome119", "chrome120", ...])` con versiones populares
- **¬øPuedo cambiar JS fingerprints?** ‚Üí NO, curl_cffi no tiene browser/JS runtime. Para eso usar Playwright stealth
- **Error HTTP/2 stream 0:** Probar remover `Content-Length` header, usar mejor proxy, o forzar HTTP/1.1

### Cloudflare Bypass
> TLS fingerprints son solo UNO de los factores. Para protecci√≥n b√°sica, TLS solo alcanza. Para protecci√≥n alta, necesit√°s: mejor IP (proxy residencial) + browser automation (Playwright)

### üéØ Aplicaci√≥n FRAVEGA
1. **Reemplazar `requests` por `curl_cffi`** en TODOS los scrapers
2. **Usar `impersonate="chrome"`** para bypass WAF Fr√°vega/Flybondi
3. **Sessions con cookies** para simular navegaci√≥n real
4. **RetryStrategy nativo** en vez de implementar retry manual
5. **AsyncSession + Semaphore** para scraping paralelo con rate limiting
6. **HTTP/3** para targets con detecci√≥n agresiva
7. **Verificar fingerprint** en tls.browserleaks.com

---

## 4Ô∏è‚É£ MCP ‚Äî Model Context Protocol

### ¬øQu√© es?
Protocolo abierto que permite a agentes de IA acceder a datos, herramientas y aplicaciones de manera estandarizada. Funciona como un "USB para AI" ‚Äî conexi√≥n universal.

### Arquitectura
```
MCP Host (tu app)
‚îú‚îÄ‚îÄ MCP Client 1 ‚Üí MCP Server (e.g., Sentry)
‚îú‚îÄ‚îÄ MCP Client 2 ‚Üí MCP Server (e.g., filesystem)
‚îî‚îÄ‚îÄ MCP Client 3 ‚Üí MCP Server (e.g., custom)
```

### Participantes
- **MCP Host:** La aplicaci√≥n AI que coordina (e.g., Claude Code, tu script)
- **MCP Client:** Componente que mantiene conexi√≥n a un MCP Server
- **MCP Server:** Programa que provee contexto/dados a clientes

### Capas
1. **Data Layer (JSON-RPC 2.0):**
   - Lifecycle management (init, negotiate, terminate)
   - Server features: **Tools** (acciones), **Resources** (datos), **Prompts** (templates)
   - Client features: Sampling (LLM completions), Elicitation (pedir info al user)
2. **Transport Layer:**
   - **Stdio:** stdin/stdout para procesos locales, sin overhead de red
   - **Streamable HTTP:** POST + Server-Sent Events, para servidores remotos

### Primitivas
- **Tools:** Funciones ejecutables (file ops, API calls, DB queries)
- **Resources:** Data sources (file contents, DB records, API responses)
- **Prompts:** Templates reutilizables (system prompts, few-shot examples)

### Build an MCP Server (Python)
```python
from mcp.server.fastmcp import FastMCP
import httpx

mcp = FastMCP("price-monitor")

@mcp.tool()
async def get_prices(product: str) -> str:
    """Get prices for a product across multiple stores."""
    # Scraping logic here
    return prices_json

@mcp.tool()
async def compare_prices(product_id: str) -> str:
    """Compare prices for a specific product."""
    # Comparison logic
    return comparison

def main():
    mcp.run(transport="stdio")

if __name__ == "__main__":
    main()
```

### Setup
```bash
uv init price-monitor
cd price-monitor
uv venv && source .venv/bin/activate
uv add "mcp[cli]" httpx
```

### Testing
- Usar **MCP Inspector** para testear tools
- Logging: NUNCA usar `print()` ‚Äî usar `logging.info()` o `print(..., file=sys.stderr)`

### üéØ Aplicaci√≥n FRAVEGA
**Para cuando el proyecto est√© m√°s maduro:**
- Crear MCP Server que exponga tools: `get_prices`, `compare_prices`, `find_deals`
- Los agentes AI podr√≠an consultar precios en tiempo real
- Permitir√≠a integraci√≥n con Claude Code, Cursor, etc.

---

## 5Ô∏è‚É£ CLAUDE CODE DOCS ‚Äî Best Practices y Workflows

### Overview
Claude Code es una herramienta de coding ag√©ntico: lee codebase, edita archivos, corre comandos. Disponible en terminal, IDE, desktop, web.

### Best Practices Extra√≠das

#### 1. Explore ‚Üí Plan ‚Üí Code
```
1. "read /src/scraper and understand how we handle sessions"  (EXPLORE)
2. "I want to add Garbarino. What files need to change? Create a plan." (PLAN)
3. "implement the scraper from your plan. write tests, run them." (IMPLEMENT)
4. "commit with a descriptive message" (COMMIT)
```

#### 2. CLAUDE.md Efectivo
- **Home folder** (`~/.claude/CLAUDE.md`): Aplica a TODAS las sesiones
- **Project root** (`./CLAUDE.md`): Check into git para compartir
- **Local** (`CLAUDE.local.md`): Para overrides personales, .gitignore
- **Child dirs:** Claude los lee on-demand

Ejemplo CLAUDE.md para nuestro proyecto:
```markdown
# Code style
- Python 3.10+, type hints obligatorios
- async por defecto para I/O
- curl_cffi en vez de requests

# Workflow
- Siempre correr tests despu√©s de cambios
- Usar circuit breaker para APIs externas
- Rate limit: max 3 requests paralelos por dominio
```

#### 3. Skills (Custom Slash Commands)
```markdown
# .claude/skills/scrape-target/SKILL.md
---
name: scrape-target
description: Scrape a new e-commerce target
---
1. Analyze the target URL structure
2. Find API endpoints (GraphQL, REST)
3. Create scraper with curl_cffi
4. Add rate limiting and error handling
5. Write tests
6. Add to monitoring rotation
```

Uso: `/scrape-target https://www.garbarino.com`

#### 4. Custom Subagents
```markdown
# .claude/agents/price-analyzer.md
---
name: price-analyzer
description: Analyzes price data for arbitrage opportunities
tools: Read, Grep, Glob, Bash
---
You are a price analysis specialist. Given product data:
- Identify lowest prices across stores
- Calculate potential margins
- Flag suspicious pricing (too low = scam, too high = error)
- Generate comparison report
```

#### 5. Session Management
- **`/clear`** entre tareas no relacionadas
- **`/compact <instructions>`** para comprimir contexto: `/compact Focus on scraping logic`
- **`/rewind`** para deshacer cambios
- **`claude --continue`** para retomar √∫ltima conversaci√≥n

#### 6. Headless Mode (Automatizaci√≥n)
```bash
# Correr an√°lisis de precios autom√°ticamente
claude -p "Analyze latest price data and generate report" --output-format json

# Fan out: procesar m√∫ltiples targets
for target in fravega garbarino musimundo; do
  claude -p "Scrape $target and save to database" \
    --allowedTools "Edit,Bash(python *)"
done
```

#### 7. Git Worktrees para Sesiones Paralelas
```bash
# Trabajar en Fr√°vega en un worktree, MercadoLibre en otro
claude -w fravega-scraper    # Crea worktree independiente
claude -w meli-scraper       # Otro worktree paralelo
```

#### 8. Failure Patterns a Evitar
| Anti-Pattern | Soluci√≥n |
|---|---|
| "Kitchen sink session" (mezclar tareas) | `/clear` entre tareas |
| Corregir error tras error sin parar | Despu√©s de 2 fallos: `/clear` y reescribir prompt |
| CLAUDE.md demasiado largo | Podar ruthlessly, usar hooks |
| "Trust then verify" (confiar sin verificar) | Siempre tests/evidence |
| Exploraci√≥n infinita | Scope narrow o usar subagents |

### Common Workflows Relevantes

#### Fix Bugs
1. Compartir el error con Claude
2. Pedir recomendaciones de fix
3. Aplicar el fix
4. Verificar con tests

#### Work with Tests
1. Identificar c√≥digo sin tests
2. Generar test scaffolding
3. Agregar edge cases
4. Correr y verificar

#### Create PRs
- Usar `/commit-push-pr` (built-in skill)
- O: `claude -p "create a pr"`

### üéØ Aplicaci√≥n FRAVEGA
1. Crear `CLAUDE.md` en ra√≠z del proyecto con convenciones
2. Crear skills custom: `/scrape-target`, `/analyze-prices`, `/find-deals`
3. Crear subagents: `price-analyzer`, `scraper-debugger`
4. Usar headless mode para scraping schedule
5. Git worktrees para trabajar en m√∫ltiples targets simult√°neos

---

## 6Ô∏è‚É£ LEAKED-SYSTEM-PROMPTS ‚Äî Colecci√≥n de Prompts de IA

### ¬øQu√© es?
Repositorio GitHub (`jujumilk3/leaked-system-prompts`) con system prompts reales filtrados de servicios AI populares.

### Prompts Disponibles
- **Perplexity AI** (m√∫ltiples versiones: 2022-2025, incluyendo GPT-4 y Claude variants)
- **Opera Aria**
- **Phind**
- **Proton Lumo**
- + 42 contributors con prompts de otros servicios

### Utilidad para FRAVEGA
- **Estudiar c√≥mo los mejores AI tools estructuran sus prompts**
- **Extraer patterns de formatting:** c√≥mo presentar datos, c√≥mo dar instrucciones claras
- **Inspiraci√≥n para PROMPTS_ARSENAL.md:** mejorar nuestros prompts de intelligence gathering
- **Entender limitaciones:** qu√© restricciones ponen los services (good for reverse engineering)

---

## üíé TOP 15 PEPITAS DE ORO ‚Äî APLICACI√ìN DIRECTA AL NEGOCIO

| # | Pepita | De D√≥nde | Impacto | C√≥digo/Pattern |
|---|--------|----------|---------|----------------|
| 1 | **curl_cffi impersonate** | curl_cffi docs | Resuelve 403 de Fr√°vega/Flybondi | `curl_cffi.get(url, impersonate="chrome")` |
| 2 | **RetryStrategy nativo** | curl_cffi | Reemplaza retry manual | `RetryStrategy(count=3, backoff="exponential")` |
| 3 | **AsyncSession + gather** | curl_cffi + async-patterns | Scraping 3-5x m√°s r√°pido | `AsyncSession()` + `asyncio.gather()` |
| 4 | **Circuit Breaker** | error-handling skill | Previene bombardeo de API ca√≠da | 3 estados: CLOSED‚ÜíOPEN‚ÜíHALF_OPEN |
| 5 | **Semaphore Rate Limit** | async-patterns skill | Control de concurrencia | `asyncio.Semaphore(3)` |
| 6 | **Session con cookies** | curl_cffi | Simula navegaci√≥n real | `with Session(impersonate="chrome") as s:` |
| 7 | **HTTP/3 para evasi√≥n** | curl_cffi fingerprint docs | Menos WAFs detectan HTTP/3 | `http_version="v3"` |
| 8 | **Excel con f√≥rmulas** | xlsx skill | Reportes din√°micos | `openpyxl` + f√≥rmulas, nunca hardcode |
| 9 | **Multi-Layer Diagnostic** | systematic-debugging | Debug eficiente de scrapers | Logs en cada frontera de componente |
| 10 | **No fix sin root cause** | systematic-debugging | Evita parches in√∫tiles | 3 intentos ‚Üí cuestionar arquitectura |
| 11 | **AGENTS.md universal** | agents.md | Compatible con 20+ AI tools | Un archivo, todas las herramientas |
| 12 | **Custom Skills** | Claude Code best practices | Automatizar tareas repetitivas | `.claude/skills/scrape-target/SKILL.md` |
| 13 | **Headless fan-out** | Claude Code best practices | Procesar m√∫ltiples targets en paralelo | `for target in ...; do claude -p ...` |
| 14 | **Graceful Degradation** | error-handling skill | Resiliencia ante fallos | `try api1 ‚Üí try api2 ‚Üí try cache ‚Üí DEFAULT` |
| 15 | **MCP Server de precios** | MCP docs | Exponer datos a AI agents | `FastMCP("price-monitor")` con tools |

---

## üéØ PLAN DE ACCI√ìN PRIORIZADO

### FASE 1 ‚Äî Inmediata (resolver el blocker de 403)
1. ‚úÖ `pip install curl_cffi`
2. ‚úÖ Refactorizar `sniffer_fravega.py` para usar `curl_cffi.Session(impersonate="chrome")`
3. ‚úÖ Agregar `RetryStrategy` nativo
4. ‚úÖ Verificar fingerprint en `tls.browserleaks.com/json`

### FASE 2 ‚Äî Esta semana (infraestructura robusta)
1. Implementar Circuit Breaker para APIs externas
2. Implementar AsyncSession + Semaphore para scraping paralelo
3. Crear Custom Exception Hierarchy (`ScrapingError`, `WAFBlockedError`, etc.)
4. Crear `AGENTS.md` y `CLAUDE.md` en ra√≠z del proyecto

### FASE 3 ‚Äî Pr√≥xima semana (automatizaci√≥n)
1. Crear skills custom: `/scrape-target`, `/analyze-prices`
2. Agregar nuevos targets (Garbarino, MercadoLibre, Musimundo)
3. Implementar Excel reporting con f√≥rmulas din√°micas
4. Setup TDD con pytest para scrapers

### FASE 4 ‚Äî Futuro (escalamiento)
1. MCP Server para exponer datos de precios
2. Docker deploy 24/7
3. CI/CD con GitHub Actions
4. Dashboard web para visualizaci√≥n

---

## ‚öñÔ∏è ¬øINSTALAR SKILLS EXISTENTES O CREAR CUSTOM?

### Recomendaci√≥n: **H√çBRIDO**

| Approach | Skills | Raz√≥n |
|----------|--------|-------|
| **Instalar directamente** | `systematic-debugging`, `verification-before-completion`, `writing-plans` | Son procesos gen√©ricos que aplican tal cual |
| **Usar como inspiraci√≥n** | `async-python-patterns`, `error-handling-patterns`, `python-testing-patterns` | Los patterns son universales pero necesitan adaptaci√≥n al contexto de scraping |
| **Crear custom** | `scrape-target`, `analyze-prices`, `find-deals`, `compare-excel` | Son espec√≠ficos de nuestro negocio, no existen en el marketplace |
| **No necesitar** | Docker, GitHub Actions, webapp-testing | Demasiado pronto, agregan complejidad innecesaria ahora |

### Para instalar una skill de skills.sh:
```bash
# Ejemplo (verificar comando exacto en skills.sh)
claude skill install obra/superpowers/systematic-debugging
```

---

*Documento generado por an√°lisis exhaustivo de 170+ chunks de contenido de 6 fuentes distintas.*
